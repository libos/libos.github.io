<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Caffe Torch TensorFlow</title>
    <meta name="description" content="Written in C++">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/monokai-sublime.css">
    <link rel="canonical" href="http://next.sh/blog/2016/03/26/lecture-12-cnn-caffe-torch-tensorflow-stanford-cs231n/">
    <link rel="alternate" type="application/rss+xml" title="Libo's Blog" href="http://next.sh/feed.xml" />
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$']],
        displayMath: [['$$','$$']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
      }
    });
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?6c9fcf4561e75bd2a6bb1aa43a6b82f1";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
    
  </head>
  <body>
    <main>
      <header class="site-header">
  <div class="container">
    <h1><a href="/blog/">Libo's <span>Blog</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
          <li><a href="/about" title="About">About</a></li>
        
          <li><a href="/blog" title="Blog">Blog</a></li>
        
        <li><a href="https://github.com/libos/libos.github.io" title="Github Repo">Github</a></li>
        <li><a href="/feed.xml" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>


      <div class="container">
        <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">Caffe Torch TensorFlow</h1>
      <p class="post-meta">March 26, 2016 &nbsp;&nbsp; 12:17</p>
      
      <div class="article-info article-info-post">
        <div class="article-tag tagcloud">
        <ul class="article-tag-list">
        
          <li class="article-tag-list-item">
            <a class="color3" href="/tags/machine learning/index.html" style="font-size: 12px;color:#fff;">Machine Learning</a>
          </li>
        
          <li class="article-tag-list-item">
            <a class="color3" href="/tags/deep learning/index.html" style="font-size: 12px;color:#fff;">Deep Learning</a>
          </li>
        
          <li class="article-tag-list-item">
            <a class="color3" href="/tags/cnn/index.html" style="font-size: 12px;color:#fff;">CNN</a>
          </li>
        
          <li class="article-tag-list-item">
            <a class="color3" href="/tags/caffe/index.html" style="font-size: 12px;color:#fff;">Caffe</a>
          </li>
        
        </ul>
        </div>
        <div class="clearfix"></div>
      </div>
    </header>
    <div class="post-content">
      <p>Written in C++</p>

<p>Has Python and MATLAB bindings</p>

<p>Good for training or finetuning feedforward model</p>

<h2 id="caffe">Caffe</h2>

<p>不需要写代码有时候==，例如ResNet</p>

<p>Document可能outdated</p>

<p>Four Major Classes：</p>

<ul>
  <li>Blob : Stores data and derivatives(weights,data,labels) [data diffs],[gpu cpu]</li>
  <li>Layer : input(bottom) blob -&gt; output(top) blob.</li>
  <li>Net : Many Layers. computes gradients via forward/backward</li>
  <li>Solver : Uses gradients to update weights</li>
</ul>

<h1 id="protocol-buffers">Protocol Buffers</h1>

<ul>
  <li>Typed Json from google</li>
  <li>Define “message types” in .proto files</li>
  <li>Serialize instances to text files .prototxt</li>
  <li>Compile classes for different languages</li>
</ul>

<h2 id="caffe-training--finetuning">Caffe: Training / Finetuning</h2>

<p>No need to write code!</p>

<ol>
  <li>Convert data (run a script)</li>
  <li>Define net (edit prototxt)</li>
  <li>Define solver (edit prototxt)</li>
  <li>Train (with pretrained weights) (run a script)</li>
</ol>

<h3 id="convert-data">Convert Data</h3>

<p>LMDB:</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>path/to/image.jpeg<span class="o">]</span> <span class="o">[</span>label<span class="o">]</span></code></pre></div>

<p>DataLayer 直接从LMDB读是最方便的</p>

<p>用<a href="https://github.com/BVLC/caffe/blob/master/tools/convert_imageset.cpp">https://github.com/BVLC/caffe/blob/master/tools/convert_imageset.cpp</a>来创建LMDB</p>

<p>使用h5py去创建HDF5</p>

<ul>
  <li>ImageDataLayer: Read from image files</li>
  <li>WindowDataLayer: For detection</li>
  <li>HDF5Layer: Read from HDF5 file</li>
  <li>From memory, using Python interface</li>
  <li>All of these are harder to use (except Python)</li>
</ul>

<h3 id="define-net">Define Net</h3>

<p><strong>.prototxt can get ugly for big models</strong></p>

<p>ResNet-152 prototxt is 6775 lines long!</p>

<p>一般可以来写Python来生成</p>

<hr />

<h4 id="prototxt">直接修改 prototxt</h4>

<p>Same name: weights copied</p>

<p>Different name: weights reinitialized</p>

<h3 id="define-solver">Define Solver</h3>

<p>If finetuning, copy existing solver. prototxt file</p>

<ul>
  <li>Change net to be your net</li>
  <li>Change snapshot_prefix to your output</li>
  <li>Reduce base learning rate (divide by 100)</li>
  <li>Maybe change max_iter and snapshot</li>
</ul>

<p>例如learning rate，可以改成几个值，然后去用</p>

<h3 id="train">Train</h3>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">./build/tools/caffe train <span class="se">\</span>
  -gpu <span class="m">0</span> <span class="se">\</span>
  -model path/to/trainval.prototxt <span class="se">\</span>
  -solver path/to/solver.prototxt <span class="se">\</span>
  -weights path/to/pretrained_weights.caffemodel</code></pre></div>

<p>-gpu -1 是CPU mode</p>

<p>-gpu all 是多GPU 并行</p>

<h2 id="model-zoo">Model Zoo</h2>

<p>AlexNet, VGG, GoogLeNet, ResNet, plus others</p>

<h2 id="caffe-python-interface">Caffe: Python Interface</h2>

<p>Not much documentation…</p>

<p>Read the code! Two most important files:</p>

<p>● caffe/python/caffe/_caffe.cpp:</p>

<p>○ Exports Blob, Layer, Net, and Solver classes</p>

<p>● caffe/python/caffe/pycaffe.py</p>

<p>○ Adds extra methods to Net class</p>

<p>Good for:</p>

<ul>
  <li>Interfacing with numpy</li>
  <li>Extract features: Run net forward</li>
  <li>Compute gradients: Run net backward (DeepDream, etc)</li>
  <li><strong>Define layers</strong> in Python with numpy (CPU only)</li>
</ul>

<p>如果定义layer的话，只能是CPU的</p>

<p><strong>注意下⬆️这里</strong></p>

<h2 id="caffe-pros--cons">Caffe Pros / Cons</h2>

<ul>
  <li>(+) Good for feedforward networks</li>
  <li>(+) Good for finetuning existing networks</li>
  <li>(+) Train models without writing any code!</li>
  <li>
    <p>(+) Python interface is pretty useful!</p>
  </li>
  <li>(-) Need to write C++ / CUDA for new GPU layers（在Caffe上写新的Layer很麻烦）</li>
  <li>(-) Not good for recurrent networks
= (-) Cumbersome for big networks (GoogLeNet, ResNet)</li>
</ul>

<h1 id="torch">Torch</h1>

<ul>
  <li>From NYU + IDIAP</li>
  <li>Written in C and Lua</li>
  <li>Used a lot a Facebook, DeepMind</li>
</ul>

<p>在GPU上运行超简单</p>

<h3 id="lua">必须用Lua来写</h3>

<p><a href="http://tylerneylon.com/a/learn-lua/">15分钟学会Lua</a></p>

<h3 id="tensors-like-numpy-arrays">Tensors like numpy arrays</h3>

<div class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="nb">require</span> <span class="s1">&#39;</span><span class="s">cutorch&#39;</span>
<span class="nb">require</span> <span class="s1">&#39;</span><span class="s">cunn&#39;</span>

<span class="kd">local</span> <span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s">torch.CudaTensor&#39;</span>

<span class="p">:</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></code></pre></div>

<p><a href="https://github.com/torch/torch7/blob/master/doc/tensor.md">https://github.com/torch/torch7/blob/master/doc/tensor.md Document</a></p>

<h2 id="torch--nn">Torch :: nn</h2>

<p>nn module lets you easily build and train neural nets</p>

<p>Build a two-layer ReLU net</p>

<h2 id="torch--cunn">Torch : cunn</h2>

<p>Import a few new packages</p>

<div class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="nb">require</span> <span class="s1">&#39;</span><span class="s">torch&#39;</span>
<span class="nb">require</span> <span class="s1">&#39;</span><span class="s">cutorch&#39;</span>
<span class="nb">require</span> <span class="s1">&#39;</span><span class="s">nn&#39;</span>
<span class="nb">require</span> <span class="s1">&#39;</span><span class="s">cunn&#39;</span></code></pre></div>

<p>Cast network and criterion</p>

<div class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="n">net</span><span class="p">:</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">crit</span><span class="p">:</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></code></pre></div>

<p>Cast data and labels</p>

<div class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="kd">local</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">):</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="kd">local</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">).</span><span class="n">random</span><span class="p">(</span><span class="n">C</span><span class="p">):</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></code></pre></div>

<h2 id="torch-optim">Torch: optim</h2>

<p>update rules: momentum, Adam, etc</p>

<p>Import optim package</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">requre <span class="s1">&#39;optim&#39;</span></code></pre></div>

<p>Write a callback function that returns loss and gradients</p>

<p>state variable holds hyperparameters, cached values, etc; pass it to adams</p>

<div class="highlight"><pre><code class="language-lua" data-lang="lua"><span class="n">optim</span><span class="p">.</span><span class="n">adm</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">state</span><span class="p">)</span></code></pre></div>

<h2 id="torch-modules">Torch: Modules</h2>

<p>Caffe has Nets and Layers;</p>

<p>Torch just has Modules</p>

<ul>
  <li>Forward / backward written in Lua using Tensor methods</li>
  <li>Modules are classes written in Lua; easy to read and write</li>
</ul>

<p>Same code runs on CPU / GPU</p>

<h2 id="container">Container</h2>

<ul>
  <li>Sequential</li>
  <li>ConcatTable</li>
  <li>ParallelTable</li>
</ul>

<p><img src="/images/2016-03-26-torch-container.png" alt="Container" /></p>

<h2 id="torch-nngraph">Torch: nngraph</h2>

<p>Use nngraph to build modules that combine their inputs in complex ways</p>

<h2 id="torch-package-management">Torch: Package Management</h2>

<p>loadcaffe: Load pretrained Caffe models: AlexNet, VGG, some others</p>

<p><a href="https://github.com/szagoruyko/loadcaffe">https://github.com/szagoruyko/loadcaffe</a></p>

<p>GoogLeNet v1: <a href="https://github.com/soumith/inception.torch">https://github.com/soumith/inception.torch</a></p>

<p>GoogLeNet v3: <a href="https://github.com/Moodstocks/inception-v3.torch">https://github.com/Moodstocks/inception-v3.torch</a></p>

<p>ResNet: <a href="https://github.com/facebook/fb.resnet.torch">https://github.com/facebook/fb.resnet.torch</a></p>

<h2 id="torch-package-management-1">Torch: Package Management</h2>

<p>like pip : luarocks</p>

<h2 id="torch-other-useful-packages">Torch: Other useful packages</h2>

<p>● torch.cudnn: Bindings for NVIDIA cuDNN kernels</p>

<p>● torch-hdf5: Read and write HDF5 files from Torch</p>

<p>● lua-cjson: Read and write JSON files from Lua,<a href="https://luarocks.org/modules/luarocks/lua-cjson">https://luarocks.org/modules/luarocks/lua-cjson</a></p>

<p>● cltorch, clnn: OpenCL backend for Torch, and port of nn</p>

<p>● torch-autograd: Automatic differentiation; sort of like more powerful nngraph, similar to Theano or TensorFlow</p>

<p>https://github.com/twitter/torch-autograd</p>

<p>● fbcunn: Facebook: FFT conv, multi-GPU (DataParallel, ModelParallel)<a href="https://github.com/facebook/fbcunn">https://github.com/facebook/fbcunn</a></p>

<h3 id="torch-typical-workflow">Torch: Typical Workflow</h3>

<p>Step 1: Preprocess data; usually use a Python script to dump data to HDF5</p>

<p>Step 2: Train a model in Lua / Torch; read from HDF5 datafile, save trained model to disk</p>

<p>Step 3: Use trained model for something, often with an evaluation script</p>

<p>https://github.com/jcjohnson/torch-rnn</p>

<p>Step 1: Preprocess data; usually use a Python script to dump data to HDF5 (https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py)</p>

<p>Step 2: Train a model in Lua / Torch; read from HDF5 datafile, save trained model to disk (https://github.com/jcjohnson/torch-rnn/blob/master/train.lua)</p>

<p>Step 3: Use trained model for something, often with an evaluation script (https://github.com/jcjohnson/torch-rnn/blob/master/sample.lua)</p>

<h1 id="torch-pros--cons">Torch: Pros / Cons</h1>

<ul>
  <li>(+) Lots of modular pieces that are easy to combine</li>
  <li>(+) Easy to write your own layer types and run on GPU</li>
  <li>(+) Most of the library code is in Lua, easy to read</li>
  <li>
    <p>(+) Lots of pretrained models!</p>
  </li>
  <li>(-) Not great for RNNs</li>
  <li>(-) Less plug-and-play than Caffe. You usually write your own training code</li>
</ul>

<h2 id="theano">Theano</h2>

<p>Python</p>

<p>http://deeplearning.net/software/theano/</p>

<p>Embracing computation graphs, symbolic computation</p>

<p>High-level wrappers: Keras, Lasagne</p>

<h3 id="compile-a-function">Compile a function</h3>

<p>在运行的时候，编译新的算法</p>

<p>produces c from x, y, z (generates code)</p>

<h3 id="symbolically">symbolically！！！！！！！！</h3>

<p>Theano computes gradients for us symbolically!</p>

<h2 id="theano-computing-gradients">Theano: Computing Gradients</h2>

<p>Problem: Shipping weights and gradients to CPU on every iteration to update…</p>

<p>解决方案：Theano: Shared Variables</p>

<p>Define weights as shared variables that persist in the graph between calls; initialize with numpy arrays</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">w1</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="mf">1e-3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="p">),</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;w1&#39;</span><span class="p">)</span></code></pre></div>

<p>Function includes an update that updates weights on every call</p>

<p>To train the net, just call function repeatedly!!!!!!!!!!!!!!</p>

<p><img src="/images/2016-03-26-Theano-Train.png" alt="repeat function" /></p>

<h2 id="theano-1">还有很多Theano的特性</h2>

<h3 id="lasagne-high-level-wrapper">Lasagne: High Level Wrapper</h3>

<p>writes the update rule for you</p>

<h3 id="keras-high-level-wrapper">Keras: High level wrapper</h3>

<p>makes common things easy to do</p>

<p>(Also supports TensorFlow backend)</p>

<h2 id="theano-pros--cons">Theano: Pros / Cons</h2>

<ul>
  <li>(+) Python + numpy</li>
  <li>(+) Computational graph is nice abstraction</li>
  <li>
    <p>(+) RNNs fit nicely in computational graph</p>
  </li>
  <li>
    <p>(+) High level wrappers (Keras, Lasagne) ease the pain</p>
  </li>
  <li>(-) Raw Theano is somewhat low-level</li>
  <li>(-) Error messages can be unhelpful</li>
  <li>(-) Large models can have long compile times</li>
  <li>(-) Much “fatter” than Torch; more magic</li>
  <li>(-) Patchy support for pretrained models</li>
</ul>

<p>编译超级慢</p>

<p>超级复杂</p>

<p>错误超级乱</p>

<h2 id="tensorflow">TensorFlow</h2>

<p>Very similar to Theano - all about computation graphs</p>

<p>Easy visualizations (TensorBoard)</p>

<p>Multi-GPU and multi-node training</p>

<h3 id="tensorflow-tensorboard">TensorFlow: Tensorboard</h3>

<p>可视化</p>

<p>卧槽！！！</p>

<p>可以做直接画图</p>

<p>而且可以直接画出整个流程图</p>

<p><img src="/images/2016-03-26-tensorflow-graph1.png" alt="TensorFlow board graph1" /></p>

<p><img src="/images/2016-03-26-tensorflow-graph2.png" alt="TensorFlow board graph2" /></p>

<h1 id="tensorflow-pros--cons">TensorFlow: Pros / Cons</h1>

<ul>
  <li>
    <p>(+) Python + numpy</p>
  </li>
  <li>
    <p>(+) Computational graph abstraction, like Theano; great for RNNs</p>
  </li>
  <li>(+) Much faster compile times than Theano</li>
  <li>
    <p>(+) Slightly more convenient than raw Theano?</p>
  </li>
  <li>
    <p>(+) TensorBoard for visualization</p>
  </li>
  <li>(+) Data AND model parallelism; best of all frameworks</li>
  <li>
    <p>(+/-) Distributed models, but not open-source yet</p>
  </li>
  <li>(-) Slower than other frameworks right now</li>
  <li>(-) Much “fatter” than Torch; more magic</li>
  <li>(-) Not many pretrained models</li>
</ul>

<h1 id="caffe-vs-torch-vs-theano-vs-tensorflow">Caffe vs Torch vs Theano vs TensorFlow</h1>

<p><img src="/images/2016-03-26-caffe-vs-torch-vs-theano-vs-tensorflow.png" alt="caffe vs torch vs theano vs tensorflow" /></p>

<hr />

<h4 id="segmentation-classify-every-pixel">Segmentation? (Classify every pixel)</h4>

<ul>
  <li>Need pretrained model (Caffe, Torch, Lasagna)</li>
  <li>Need funny loss function</li>
  <li>If loss function exists in Caffe: Use Caffe</li>
  <li>If you want to write your own loss: Use Torch</li>
</ul>

<h4 id="object-detection">Object Detection?</h4>

<p>-&gt; Use Caffe + Python or Torch</p>

<h4 id="language-modeling-with-new-rnn-structure">Language modeling with new RNN structure?</h4>

<p>-&gt; Use Theano or TensorFlow</p>

<h4 id="implement-batchnorm">Implement BatchNorm?</h4>

<p>-&gt; Implement efficient backward pass? Use Torch</p>

<h1 id="recommendation">Recommendation</h1>

<ul>
  <li>
    <p>Feature extraction / finetuning existing models: Use Caffe</p>
  </li>
  <li>
    <p>Complex uses of pretrained models: Use Lasagne or Torch</p>
  </li>
  <li>
    <p>Write your own layers: Use Torch</p>
  </li>
  <li>
    <p>Crazy RNNs: Use Theano or Tensorflow</p>
  </li>
  <li>
    <p>Huge model, need model parallelism: Use TensorFlow</p>
  </li>
</ul>

<p>neon™</p>

<p>The Fastest Deep Learning Framework</p>

<p>neon™ is open source and can be deployed on CPUs, GPUs or custom Nervana hardware. It supports all the commonly used models including convnets, MLPs, RNNs, LSTMs and autoencoder</p>


    </div>

    
  <hr>

  <aside id="comments" class="disqus">

    <div class="container">
      <h3><i class="icon icon-comments-o"></i> Comments</h3>
      <div id="disqus_thread"></div>

      <script type="text/javascript">
        var disqus_shortname = 'libos';
        var disqus_identifier = '/blog/2016/03/26/lecture-12-cnn-caffe-torch-tensorflow-stanford-cs231n';
        var disqus_title = 'Caffe Torch TensorFlow';
        var disqus_url = 'http://next.sh/blog/2016/03/26/lecture-12-cnn-caffe-torch-tensorflow-stanford-cs231n';
        /*var disqus_developer = 1;*/
        var disqus_config = function () {
          this.page.url = disqus_url; // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = disqus_identifier; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>

      <noscript>
        Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
      </noscript>
    </div>

  </aside>

  </div>

</article>

      </div>

      <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="http://github.com/libos" target="_blank"><i class="icon icon-github"></i></a></li>
  <li><a href="http://twitter.com/LiberSnail" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="http://facebook.com/gslipt" target="_blank"><i class="icon icon-facebook"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2016 All rights reserved.</small>
    </p>
  </div>
</footer>


      

      <script src="/js/jquery-1.11.3.min.js"></script>
      <script src="/js/highlight.pack.js"></script>
      <script>
      hljs.initHighlightingOnLoad();
      $(document).ready(function() {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart',function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
      </script>
    </main>
  </body>
</html>
